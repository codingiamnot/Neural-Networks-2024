{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPHhwAzujFLYQRJLUBUMBry",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/codingiamnot/Neural-Networks-2024/blob/main/Lab03/tema3_rn_drop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "id": "5jLDUJ5QSP2t"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "def download_mnist(is_train: bool):\n",
        "    dataset = MNIST(root=\"./data\",\n",
        "        transform=lambda x: np.array(x).flatten(),\n",
        "        download=True,\n",
        "        train=is_train)\n",
        "\n",
        "    mnist_data = []\n",
        "    mnist_labels = []\n",
        "\n",
        "    for image, label in dataset:\n",
        "        mnist_data.append(image)\n",
        "        mnist_labels.append(label)\n",
        "\n",
        "    return mnist_data, mnist_labels\n",
        "\n",
        "\n",
        "train_X, train_Y = download_mnist(True)\n",
        "test_X, test_Y = download_mnist(False)\n",
        "\n",
        "train_X = np.array(train_X)\n",
        "train_Y = np.array(train_Y)\n",
        "test_X = np.array(test_X)\n",
        "test_Y = np.array(test_Y)\n",
        "\n",
        "\n",
        "#normalize the input data\n",
        "\n",
        "train_X = np.divide(train_X, 256)\n",
        "test_X = np.divide(test_X, 256)\n",
        "\n",
        "#conver ouput to one hot encoding\n",
        "\n",
        "def oneHot(value):\n",
        "  ans = np.zeros(10,)\n",
        "  ans[value] = 1\n",
        "  return ans\n",
        "\n",
        "train_Y = np.array([oneHot(value) for value in train_Y])\n",
        "test_Y = np.array([oneHot(value) for value in test_Y])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "def sigmoid(x):\n",
        "  if x >= 100:\n",
        "    return 1\n",
        "\n",
        "  if x <= -100:\n",
        "    return 0\n",
        "\n",
        "  return 1 / (1 + math.exp(-x))\n",
        "\n",
        "sigmoid_np = np.vectorize(sigmoid)\n",
        "\n",
        "def sigmoid_deriv(x):\n",
        "  value = sigmoid(x)\n",
        "  return value * (1-value)\n",
        "\n",
        "sigmoid_deriv_np = np.vectorize(sigmoid_deriv)\n",
        "\n",
        "\n",
        "\n",
        "def softmax(x):\n",
        "  return np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)\n",
        "\n",
        "\n",
        "class NeuralNetwork:\n",
        "\n",
        "  def __init__(self, hidden_nodes=100, learning_rate=0.01):\n",
        "    self.w12 = np.random.randn(784, hidden_nodes)\n",
        "    self.w23 = np.random.randn(hidden_nodes, 10)\n",
        "    self.b2 = np.random.randn(hidden_nodes, )\n",
        "    self.b3 = np.random.randn(10, )\n",
        "    self.learning_rate = learning_rate\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = np.dot(x, self.w12) + self.b2\n",
        "    x = sigmoid_np(x)\n",
        "    x = np.dot(x, self.w23) + self.b3\n",
        "    x = softmax(x)\n",
        "    return x\n",
        "\n",
        "  def backpropagation(self, x, y, p_on):\n",
        "    active_hidden = np.random.choice([0, 1],\n",
        "                                     size=(self.w12.shape[1],),\n",
        "                                     p=[1-p_on, p_on])\n",
        "\n",
        "    active_hidden = np.tile(active_hidden, (x.shape[0], 1))\n",
        "\n",
        "    #values of the hidden layer\n",
        "    values2a = np.dot(x, self.w12) + self.b2\n",
        "    values2z = sigmoid_np(values2a)\n",
        "\n",
        "    #deactivate nodes\n",
        "    values2z = np.multiply(values2z, active_hidden)\n",
        "\n",
        "\n",
        "    #values of the output layer\n",
        "    values3 = np.dot(values2z, self.w23) + self.b3\n",
        "    values3 = softmax(values3)\n",
        "\n",
        "    gradient_values3 = y - values3\n",
        "    gradient_w23 = np.dot(values2z.T, gradient_values3)\n",
        "    gradient_b3 = np.sum(gradient_values3, axis=0)\n",
        "\n",
        "    #gradient after activation function\n",
        "    gradient_values2 = np.dot(gradient_values3, self.w23.T)\n",
        "    #drop deactivated nodes\n",
        "    gradient_values2 = np.multiply(gradient_values2, active_hidden)\n",
        "    #gradient before activation function\n",
        "    gradient_values2 = np.multiply(sigmoid_deriv_np(values2a), gradient_values2)\n",
        "\n",
        "    gradient_w12 = np.dot(x.T, gradient_values2)\n",
        "    gradient_b2 = np.sum(gradient_values2, axis=0)\n",
        "\n",
        "\n",
        "    self.w23 += self.learning_rate * gradient_w23\n",
        "    self.b3 += self.learning_rate * gradient_b3\n",
        "\n",
        "    self.w12 += self.learning_rate * gradient_w12\n",
        "    self.b2 += self.learning_rate * gradient_b2\n",
        "\n",
        "\n",
        "  def train(self, x, y, epochs=1, batch_size=100, p_on=0.5):\n",
        "\n",
        "    while epochs > 0:\n",
        "      epochs -= 1\n",
        "\n",
        "      print(epochs, \"epochs left\")\n",
        "\n",
        "      permutation = np.random.permutation(x.shape[0])\n",
        "      x = x[permutation]\n",
        "      y = y[permutation]\n",
        "\n",
        "      for i in range(0, x.shape[0], batch_size):\n",
        "        batch_x = x[i:i+batch_size]\n",
        "        batch_y = y[i:i+batch_size]\n",
        "\n",
        "        self.backpropagation(batch_x, batch_y, p_on)\n",
        "\n",
        "\n",
        "  def accuracy(self, x, y):\n",
        "    pred_y = self.forward(x)\n",
        "\n",
        "    labels = np.argmax(y, axis=1)\n",
        "    predictions = np.argmax(pred_y, axis=1)\n",
        "    return np.mean(predictions == labels)"
      ],
      "metadata": {
        "id": "ZS45FWiVTgTO"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "model = NeuralNetwork(learning_rate=0.02)\n",
        "print(\"before training\", model.accuracy(train_X, train_Y), model.accuracy(test_X, test_Y))\n",
        "\n",
        "time_start = time.time()\n",
        "model.train(train_X, train_Y, epochs=20, p_on=0.8)\n",
        "time_end = time.time()\n",
        "\n",
        "print((time_end - time_start) / 60)\n",
        "\n",
        "print(\"after training\", model.accuracy(train_X, train_Y), model.accuracy(test_X, test_Y))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2cKoTWzgV3U",
        "outputId": "227cec2d-7bf0-47eb-f061-8bc8f1d95a8d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before training 0.09636666666666667 0.0949\n",
            "19 epochs left\n",
            "18 epochs left\n",
            "17 epochs left\n",
            "16 epochs left\n",
            "15 epochs left\n",
            "14 epochs left\n",
            "13 epochs left\n",
            "12 epochs left\n",
            "11 epochs left\n",
            "10 epochs left\n",
            "9 epochs left\n",
            "8 epochs left\n",
            "7 epochs left\n",
            "6 epochs left\n",
            "5 epochs left\n",
            "4 epochs left\n",
            "3 epochs left\n",
            "2 epochs left\n",
            "1 epochs left\n",
            "0 epochs left\n",
            "4.97994441986084\n",
            "after training 0.9801 0.9637\n"
          ]
        }
      ]
    }
  ]
}